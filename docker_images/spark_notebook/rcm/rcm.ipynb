{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa8402b-5f97-4e04-8684-49aabbbd27f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"pyspark-rdd-demo-{}\".format(datetime.today()))\n",
    "        .master(\"spark://spark-master:7077\")      \n",
    "        .getOrCreate())\n",
    "# spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fbbe770-1bac-4ac4-949a-52683b7c230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = spark.read.format(\"parquet\").load(\"s3a://warehouse/gold/tiki/products.parquet\")\n",
    "users = spark.read.format(\"parquet\").load(\"s3a://warehouse/gold/tiki/users.parquet\")\n",
    "reviews = spark.read.format(\"parquet\").load(\"s3a://warehouse/gold/tiki/reviews.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a258b8-698e-4185-ae16-26d6d7ea02f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'underthesea'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_info\u001b[39m\u001b[38;5;124m'\u001b[39m, process_text(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Show the results\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_info\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nModuleNotFoundError: No module named 'underthesea'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from underthesea import word_tokenize\n",
    "import re\n",
    "\n",
    "@udf(StringType())  # Specify return type as String\n",
    "def process_text(document):\n",
    "    # Change to lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Remove HTTP links (using regular expression)\n",
    "    document = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', '', document)\n",
    "    \n",
    "    # Remove line breaks (replace with space)\n",
    "    document = re.sub(r'[\\r\\n]+', ' ', document)\n",
    "    \n",
    "    # Replace '/' and ',' with space\n",
    "    document = document.replace('/', ' ').replace(',', ' ')\n",
    "    \n",
    "    # Remove punctuations using regular expression\n",
    "    document = re.sub(r'[^\\w\\s]', '', document)\n",
    "    \n",
    "    # Remove extra spaces (replace multiple spaces with a single space)\n",
    "    document = re.sub(r'[\\s]{2,}', ' ', document)\n",
    "    \n",
    "    # Tokenize text using word_tokenize from underthesea\n",
    "    document = word_tokenize(document, format=\"text\")\n",
    "    \n",
    "    return document\n",
    "\n",
    "# Assuming `products` is your DataFrame\n",
    "df = products\n",
    "\n",
    "# Create the 'info' column by concatenating product_name, description, and specifications\n",
    "df = df.withColumn('info', df['product_name'] + ' ' + df['description'] + ' ' + df['specifications'])\n",
    "\n",
    "# Apply the UDF to the 'info' column\n",
    "df = df.withColumn('processed_info', process_text(df['info']))\n",
    "\n",
    "# Show the results\n",
    "df.select('processed_info').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fc700fd4-e4ea-4306-a4d5-1e43063dabe6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'content'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dictionary, tfidf, index\n\u001b[1;32m     41\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m load_stopword(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvietnamese-stopwords.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mgensim_rcm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproducts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m tfidf \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mTfidfModel\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf_model.gensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     44\u001b[0m index \u001b[38;5;241m=\u001b[39m similarities\u001b[38;5;241m.\u001b[39mSparseMatrixSimilarity\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity_index.gensim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[58], line 12\u001b[0m, in \u001b[0;36mgensim_rcm\u001b[0;34m(products, stop_words)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgensim_rcm\u001b[39m(products, stop_words):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Tokenize the product content\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     info \u001b[38;5;241m=\u001b[39m [[text \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39msplit()] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mproducts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Create a dictionary mapping words to unique IDs\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     dictionary \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(info)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'content'"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "from gensim import similarities\n",
    "\n",
    "def load_stopword(STOP_WORDS):\n",
    "    with open(STOP_WORDS, 'r', encoding = 'utf-8') as file:\n",
    "        stop_words = file.read()\n",
    "    stop_words = stop_words.split('\\n')\n",
    "    return stop_words\n",
    "    \n",
    "def gensim_rcm(products, stop_words):\n",
    "    # Tokenize the product content\n",
    "    info = [[text for text in x.split()] for x in products['content']]\n",
    "    \n",
    "    # Create a dictionary mapping words to unique IDs\n",
    "    dictionary = corpora.Dictionary(info)\n",
    "    \n",
    "    # Filter out stop words and words that appear only once in the corpus\n",
    "    stop_ids = [dictionary.token2id[stopword] for stopword in stop_words if stopword in dictionary.token2id]\n",
    "    once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "    dictionary.filter_tokens(stop_ids + once_ids)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    # Convert the product content into a Bag of Words (BoW) format\n",
    "    corpus = [dictionary.doc2bow(text) for text in info]\n",
    "    \n",
    "    # Create a TF-IDF model from the corpus\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    \n",
    "    # The number of unique features (terms) in the dictionary\n",
    "    feature_cnt = len(dictionary.token2id)\n",
    "    \n",
    "    # Convert the TF-IDF corpus into a sparse matrix for memory efficiency\n",
    "    index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features=feature_cnt)\n",
    "    \n",
    "    # Return the dictionary, TF-IDF model, and similarity index for further use\n",
    "    dictionary.save('dictionary.gensim')\n",
    "    tfidf.save('tfidf_model.gensim')\n",
    "    index.save('similarity_index.gensim')\n",
    "    return dictionary, tfidf, index\n",
    "\n",
    "stop_words = load_stopword(\"vietnamese-stopwords.txt\")\n",
    "_ = gensim_rcm(products.toPandas(), stop_words)\n",
    "tfidf = models.TfidfModel.load('tfidf_model.gensim')\n",
    "index = similarities.SparseMatrixSimilarity.load('similarity_index.gensim')\n",
    "dictionary = corpora.Dictionary.load('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "147d1b1f-a5cb-494c-9499-6895efff7c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chổi',\n",
       " 'vệ_sinh',\n",
       " 'rửa',\n",
       " 'xe',\n",
       " 'ô_tô',\n",
       " 'cây',\n",
       " 'lau',\n",
       " 'nhà',\n",
       " 'đặc_biệt',\n",
       " 'd...']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "product_ID = 276854528\n",
    "n = 5\n",
    "product_result= df[df.product_id == product_ID].head(1)   \n",
    "view_product = product_result['content'].to_string(index = False)\n",
    "view_product = view_product.split()\n",
    "view_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3fcefc7c-0fdf-4922-a567-2cdd5365716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = dictionary.doc2bow(view_product)\n",
    "sim = index[tfidf[bow_vector]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba796886-31d2-446d-afff-f227009bb2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3804</th>\n",
       "      <td>203428870</td>\n",
       "      <td>Chổi Lau Quét Bụi Xe Ô Tô, Chổi Lau Vệ Sinh Xe...</td>\n",
       "      <td>3804</td>\n",
       "      <td>0.770375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5026</th>\n",
       "      <td>275182599</td>\n",
       "      <td>Chổi Vệ Sinh Rửa Xe Ô Tô - Cây Lau Nhà Đặc Biệ...</td>\n",
       "      <td>5026</td>\n",
       "      <td>0.699240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9077</th>\n",
       "      <td>192706518</td>\n",
       "      <td>Khăn lau xe ô tô đa năng microfiber màu vàng -...</td>\n",
       "      <td>9077</td>\n",
       "      <td>0.613022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6932</th>\n",
       "      <td>71150931</td>\n",
       "      <td>Chổi Quét Bụi Ô TÔ Đa Năng NB 64cm</td>\n",
       "      <td>6932</td>\n",
       "      <td>0.601728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4328</th>\n",
       "      <td>270677877</td>\n",
       "      <td>Khăn lau xe ô tô đa năng microfiber xám không ...</td>\n",
       "      <td>4328</td>\n",
       "      <td>0.595468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_id                                       product_name    id  \\\n",
       "3804   203428870  Chổi Lau Quét Bụi Xe Ô Tô, Chổi Lau Vệ Sinh Xe...  3804   \n",
       "5026   275182599  Chổi Vệ Sinh Rửa Xe Ô Tô - Cây Lau Nhà Đặc Biệ...  5026   \n",
       "9077   192706518  Khăn lau xe ô tô đa năng microfiber màu vàng -...  9077   \n",
       "6932    71150931                 Chổi Quét Bụi Ô TÔ Đa Năng NB 64cm  6932   \n",
       "4328   270677877  Khăn lau xe ô tô đa năng microfiber xám không ...  4328   \n",
       "\n",
       "         score  \n",
       "3804  0.770375  \n",
       "5026  0.699240  \n",
       "9077  0.613022  \n",
       "6932  0.601728  \n",
       "4328  0.595468  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_id = []\n",
    "list_score = []\n",
    "for i in range(len(sim)):\n",
    "    list_id.append(i)\n",
    "    list_score.append(sim[i])\n",
    "\n",
    "df_result = pd.DataFrame({'id': list_id,\n",
    "                            'score': list_score})\n",
    "\n",
    "five_highest_score = df_result.sort_values(by = 'score', ascending = False).head(n +1)\n",
    "idToList = list(five_highest_score['id'])\n",
    "\n",
    "product_find = df[df.index.isin(idToList)]\n",
    "result = product_find[['product_id','product_name']]\n",
    "result = pd.concat([result, five_highest_score], axis = 1).sort_values(by = 'score', ascending = False)\n",
    "result = result[result.product_id != product_ID]\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
