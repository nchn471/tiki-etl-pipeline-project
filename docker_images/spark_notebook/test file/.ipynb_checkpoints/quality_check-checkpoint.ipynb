{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21e0faba-4470-4d10-8067-5283ba7f5f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (SparkSession.builder.appName(\"pyspark-rdd-demo-{}\".format(datetime.today()))\n",
    "        .master(\"spark://spark-master:7077\")      \n",
    "        .getOrCreate())\n",
    "\n",
    "# spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d485289-eada-4f0e-a52f-a7e24a2037b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o148.load.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:833)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:63)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m silver_products \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3a://warehouse/silver/tiki/products.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o148.load.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:833)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2707)\n\tat org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:63)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:497)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:79)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:186)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "silver_products = spark.read.format(\"parquet\").load(\"s3a://warehouse/silver/tiki/products.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe3e611-3b6c-4693-a302-9b214bf59608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        []\n",
       "1        []\n",
       "2        []\n",
       "3        []\n",
       "4        []\n",
       "         ..\n",
       "11120    []\n",
       "11121    []\n",
       "11122    []\n",
       "11123    []\n",
       "11124    []\n",
       "Name: authors, Length: 11125, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silver_products.toPandas()['authors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51b52bbe-a63d-4448-a24f-7987bfdcd2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "exploded_authors = silver_products.withColumn(\"author\", explode(col(\"authors\"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbf36939-daa2-4022-bf08-58683ab3e9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        {'name': None, 'id': 21038}\n",
       "1        {'name': None, 'id': 63949}\n",
       "2         {'name': None, 'id': 1178}\n",
       "3       {'name': None, 'id': 143619}\n",
       "4       {'name': None, 'id': 149823}\n",
       "                   ...              \n",
       "631    {'name': None, 'id': 8249934}\n",
       "632      {'name': None, 'id': 25537}\n",
       "633    {'name': None, 'id': 7476405}\n",
       "634    {'name': None, 'id': 8249934}\n",
       "635     {'name': None, 'id': 611193}\n",
       "Name: author, Length: 636, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded_authors.toPandas()['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "207a8406-832b-404e-8ad5-b27eed6d4b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[author_id: bigint, author_name: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "select_columns = [\n",
    "    'category_id', 'product_id', 'seller_id', 'brand_id', 'product_name', 'description','specifications',\n",
    "    'breadcrumbs','original_price', 'discount', 'price', 'discount_rate', 'quantity_sold', 'rating_average',\n",
    "    'review_count', 'day_ago_created', 'product_url', 'is_authentic', 'is_freeship_xtra',\n",
    "    'is_top_deal', 'return_reason', 'inventory_type', 'warranty_period', 'warranty_type', 'warranty_location'\n",
    "]\n",
    "\n",
    "gold_products = silver_products.select(*select_columns)\n",
    "\n",
    "exploded_authors = silver_products.withColumn(\"author_id\", explode(col(\"authors_id\"))) \\\n",
    "                                  .withColumn(\"author_name\", explode(col(\"authors_name\")))\n",
    "\n",
    "gold_authors = exploded_authors.select(\"author_id\", \"author_name\") \\\n",
    "                               .drop_duplicates([\"author_id\"]) \\\n",
    "                               .dropna()\n",
    "\n",
    "product_author_pairs = silver_products.withColumn(\"author_id\", explode(col(\"authors_id\"))) \\\n",
    "                                      .withColumn(\"author_name\", explode(col(\"authors_name\"))) \\\n",
    "                                      .select(\"product_id\", \"seller_id\", \"author_id\")\n",
    "gold_products_authors = product_author_pairs.drop_duplicates()\n",
    "\n",
    "gold_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "067f0609-0c48-42f0-8cd7-cd047f02549f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Daniel Goleman, Morten Hansen, Harvard Busine...\n",
       "Name: authors_name, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary SQL view\n",
    "silver_products.createOrReplaceTempView(\"silver_products\")\n",
    "\n",
    "# Run SQL query\n",
    "result = spark.sql(\"SELECT * FROM silver_products WHERE product_id = 11505582\")\n",
    "\n",
    "# Show the result\n",
    "result.toPandas()['authors_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "743acebd-a6a5-47b9-b6e9-5114bb61edcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+\n",
      "|author_id|   author_name|\n",
      "+---------+--------------+\n",
      "|     3407|Daniel Goleman|\n",
      "|   144149|Daniel Goleman|\n",
      "|   172475|Daniel Goleman|\n",
      "|   172509|Daniel Goleman|\n",
      "|   304427|Daniel Goleman|\n",
      "|   309375|Daniel Goleman|\n",
      "|   310039|Daniel Goleman|\n",
      "+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary SQL view\n",
    "gold_authors.createOrReplaceTempView(\"gold_authors_view\")\n",
    "\n",
    "# Run SQL query\n",
    "result = spark.sql(\"SELECT * FROM gold_authors_view WHERE author_name = 'Daniel Goleman'\")\n",
    "\n",
    "# Show the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd62195e-a749-4f63-af78-f0e264bd0366",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = spark.read.format(\"parquet\").load(\"s3a://warehouse/gold/tiki/users.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eacd6ef-e30f-4811-a100-7e10fa375787",
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers = spark.read.format(\"parquet\").load(\"s3a://warehouse/silver/tiki/sellers.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8cf6f2-4253-431c-928f-0ca011a7113e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['OFFICIAL_STORE', 'NONE', 'TRUSTED_STORE'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sellers.toPandas()['store_level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4902335-c6ae-4286-a43f-3a75c1d55e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, when, from_unixtime, udf, explode\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e6c212-c539-4720-9f40-ea208524b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = spark.read.format(\"parquet\").load(\"s3a://warehouse/gold/tiki/products.parquet\")\n",
    "# products = products.toPandas()\n",
    "# products.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a124af0d-5639-40bc-9267-a856cd3f27cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "return_reason\n",
       "Bất cứ lý do gì     9455\n",
       "Sản phẩm hư hỏng    1328\n",
       "Không đổi trả        342\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col, isnan\n",
    "\n",
    "products = products.withColumn(\n",
    "    \"warranty_type\",\n",
    "    when(isnan(col(\"warranty_type\")) | col(\"warranty_type\").isNull(), \"Không bảo hành\")\n",
    "    .otherwise(col(\"warranty_type\"))\n",
    ")\n",
    "\n",
    "products = products.withColumn(\n",
    "    \"warranty_location\",\n",
    "    when(isnan(col(\"warranty_location\")) | col(\"warranty_location\").isNull(), \"Không bảo hành\")\n",
    "    .otherwise(col(\"warranty_location\"))\n",
    ")\n",
    "products = products.withColumn(\n",
    "    \"return_reason\",\n",
    "    when(col(\"return_reason\") == \"any_reason\", \"Bất cứ lý do gì\")\n",
    "    .when(col(\"return_reason\") == \"defective_product\", \"Sản phẩm hư hỏng\")\n",
    "    .when((col(\"return_reason\") == \"no_return\") | col(\"return_reason\").isNull(), \"Không đổi trả\")\n",
    "    .otherwise(col(\"return_reason\"))\n",
    ")\n",
    "\n",
    "# Hiển thị kết quả\n",
    "products.toPandas()['return_reason'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f89c767c-0cc4-4314-ac34-6fa03442d7d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DataFrame.fillna() got an unexpected keyword argument 'subset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m products \u001b[38;5;241m=\u001b[39m \u001b[43mproducts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKhông bảo hành\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwarranty_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwarranty_location\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# products_spark_df = products_spark_df.fillna(\"no_return\", subset=[\"return_reason\"])\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# products_spark_df = products_spark_df.fillna(0, subset=[\"quantity_sold\"])\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: DataFrame.fillna() got an unexpected keyword argument 'subset'"
     ]
    }
   ],
   "source": [
    "products = products.filsplna(\"Không bảo hành\", subset=[\"warranty_type\", \"warranty_location\"])\n",
    "# products_spark_df = products_spark_df.fillna(\"no_return\", subset=[\"return_reason\"])\n",
    "# products_spark_df = products_spark_df.fillna(0, subset=[\"quantity_sold\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d2f9d1f-35ba-413b-bf3f-1c9d8d1b7666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warranty_type\n",
       "Không bảo hành    7136\n",
       "Điện tử           1528\n",
       "Hóa đơn           1291\n",
       "Phiếu bảo hành     774\n",
       "Tem bảo hành       396\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_pd = products.toPandas()\n",
    "\n",
    "# products_pd[products_pd['warranty_type'] == 'NaN']\n",
    "products_pd['warranty_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ce61ab0-2113-412a-9d02-59c668d2839a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 69309 entries, 0 to 69308\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   product_id  69309 non-null  int64 \n",
      " 1   seller_id   69309 non-null  int64 \n",
      " 2   image_url   69309 non-null  object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "img_url = spark.read.format(\"parquet\").load(\"s3a://warehouse/gold/tiki/images_url.parquet\")\n",
    "img_url = img_url.toPandas()\n",
    "img_url.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e2a5cf-a6c8-4a6d-9362-a4f56d7e7a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122012</td>\n",
       "      <td>1</td>\n",
       "      <td>https://salt.tikicdn.com/ts/product/24/85/1a/7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>122012</td>\n",
       "      <td>1</td>\n",
       "      <td>https://salt.tikicdn.com/media/catalog/product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>122012</td>\n",
       "      <td>1</td>\n",
       "      <td>https://salt.tikicdn.com/media/catalog/product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122012</td>\n",
       "      <td>1</td>\n",
       "      <td>https://salt.tikicdn.com/media/catalog/product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>122012</td>\n",
       "      <td>1</td>\n",
       "      <td>https://salt.tikicdn.com/media/catalog/product...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  seller_id                                          image_url\n",
       "0      122012          1  https://salt.tikicdn.com/ts/product/24/85/1a/7...\n",
       "1      122012          1  https://salt.tikicdn.com/media/catalog/product...\n",
       "2      122012          1  https://salt.tikicdn.com/media/catalog/product...\n",
       "3      122012          1  https://salt.tikicdn.com/media/catalog/product...\n",
       "4      122012          1  https://salt.tikicdn.com/media/catalog/product..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_url.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbed778f-addf-4a76-9030-d86ae1e2c8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 133909 entries, 0 to 133908\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   user_id       133909 non-null  int64  \n",
      " 1   user_name     133054 non-null  object \n",
      " 2   avatar_url    133909 non-null  object \n",
      " 3   joined_day    133909 non-null  int32  \n",
      " 4   joined_time   133869 non-null  object \n",
      " 5   total_review  133869 non-null  float64\n",
      " 6   total_thank   133869 non-null  float64\n",
      "dtypes: float64(2), int32(1), int64(1), object(3)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "users = users.toPandas()\n",
    "users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a5e6c04-2d80-47cb-aad0-40367a233689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avatar_url\n",
       "//tiki.vn/assets/img/avatar.png                                                                                                                                                                                                                                       97664\n",
       "http://s120.avatar.talk.zdn.vn/default                                                                                                                                                                                                                                   17\n",
       "https://s120.avatar.talk.zdn.vn/default                                                                                                                                                                                                                                  11\n",
       "https://scontent-iad3-1.xx.fbcdn.net/v/t31.0-1/c59.0.200.200a/p200x200/10733713_10150004552801937_4553731092814901385_o.jpg?_nc_cat=1&_nc_sid=12b3be&_nc_ohc=kbRb-3obZ7MAX9TZwXo&_nc_ht=scontent-iad3-1.xx&oh=53c874b14b5499a854349017c7a0b7f6&oe=5EB5F4F5                2\n",
       "https://scontent.xx.fbcdn.net/v/t1.0-1/c59.0.200.200a/p200x200/10645251_10150004552801937_4553731092814901385_n.jpg?_nc_cat=1&_nc_ohc=MmqVv0KK-pYAQn91U9qlCrzC7Nkttsfka0CXqIxU-_lDjdfjyL7YNnQnQ&_nc_ht=scontent.xx&oh=a5f46d3b20dcb81c798a260189ca12cd&oe=5E6E4C12        2\n",
       "                                                                                                                                                                                                                                                                      ...  \n",
       "https://platform-lookaside.fbsbx.com/platform/profilepic/?asid=2165237913488814&height=200&width=200&ext=1571229060&hash=AeRdHEQ7jBMO-FA_                                                                                                                                 1\n",
       "https://graph.facebook.com/v3.3/748213385609581/picture?type=large&return_ssl_resources=1                                                                                                                                                                                 1\n",
       "https://platform-lookaside.fbsbx.com/platform/profilepic/?asid=344422659771148&height=200&width=200&ext=1571227296&hash=AeSnz6_tIjbmIfyL                                                                                                                                  1\n",
       "https://graph.facebook.com/v3.3/1288775367970187/picture?type=large&return_ssl_resources=1                                                                                                                                                                                1\n",
       "https://salt.tikicdn.com/cache/512x512/ts/avatar/a3/9c/e1/81bd2927e340c44c8c3d330cdb6f0c70.png                                                                                                                                                                            1\n",
       "Name: count, Length: 36218, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users['avatar_url'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b205db7c-5762-4731-a400-1c3c5ca5af73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
